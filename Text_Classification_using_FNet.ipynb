{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Setup**"
      ],
      "metadata": {
        "id": "oU50QOomfQrZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW6d0t-KeyoM"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade keras-nlp\n",
        "!pip install -q --upgrade keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_nlp\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "keras.utils.set_random_seed(42)"
      ],
      "metadata": {
        "id": "qMhZsvxMfj79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EPOCHS = 3\n",
        "MAX_SEQUENCE_LENGTH = 512\n",
        "VOCAB_SIZE = 15000\n",
        "\n",
        "EMBED_DIM = 128\n",
        "INTERMEDIATE_DIM = 512"
      ],
      "metadata": {
        "id": "ih_p8KLCfj5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "Gm7DdShTgGRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8Z83Z0zfj2o",
        "outputId": "749fecf0-fe1c-49b5-b627-1ca6e6c0dd9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-13 13:10:58--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  13.1MB/s    in 5.8s    \n",
            "\n",
            "2024-09-13 13:11:04 (13.9 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir(\"./aclImdb\"))\n",
        "print(os.listdir(\"./aclImdb/train\"))\n",
        "print(os.listdir(\"./aclImdb/test\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn9XhkPyfjz4",
        "outputId": "647460a6-353c-4695-c155-de07cf307e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train', 'README', 'imdb.vocab', 'imdbEr.txt', 'test']\n",
            "['urls_pos.txt', 'neg', 'unsup', 'unsupBow.feat', 'urls_neg.txt', 'pos', 'labeledBow.feat', 'urls_unsup.txt']\n",
            "['urls_pos.txt', 'neg', 'urls_neg.txt', 'pos', 'labeledBow.feat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "MXBjMbpGgYnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=42,\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=42,\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXJPvMhwgcHQ",
        "outputId": "3571fad7-c1c6-4fa3-f623-a39ce9b29bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text_batch, label_batch in train_ds.take(1):\n",
        "    for i in range(3):\n",
        "        print(text_batch.numpy()[i])\n",
        "        print(label_batch.numpy()[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kd4WJHJ_grv4",
        "outputId": "ecb3217d-47fc-4f67-dcdf-af5129f552f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'An illegal immigrant resists the social support system causing dire consequences for many. Well filmed and acted even though the story is a bit forced, yet the slow pacing really sets off the conclusion. The feeling of being lost in the big city is effectively conveyed. The little person lost in the big society is something to which we can all relate, but I cannot endorse going out of your way to see this movie.'\n",
            "0\n",
            "b\"To get in touch with the beauty of this film pay close attention to the sound track, not only the music, but the way all sounds help to weave the imagery. How beautifully the opening scene leading to the expulsion of Gino establishes the theme of moral ambiguity! Note the way music introduces the characters as we are led inside Giovanna's marriage. Don't expect to find much here of the political life of Italy in 1943. That's not what this is about. On the other hand, if you are susceptible to the music of images and sounds, you will be led into a word that reaches beyond neo-realism. By the end of the film we there are moments Antonioni-like landscape that has more to do with the inner life of the characters than with real places. This is one of my favorite Visconti films.\"\n",
            "1\n",
            "b'\"Hollywood Hotel\" has relationships to many films like \"Ella Cinders\" and \"Merton of the Movies\" about someone winning a contest including a contract to make films in Hollywood, only to find the road to stardom either paved with pitfalls or non-existent. In fact, as I was watching it tonight, on Turner Classic Movies, I was considering whether or not the authors of the later musical classic \"Singing In The Rain\" may have taken some of their ideas from \"Hollywood Hotel\", most notably a temperamental leading lady star in a movie studio and a conclusion concerning one person singing a film score while another person got the credit by mouthing along on screen.<br /><br />\"Hollywood Hotel\" is a fascinating example of movie making in the 1930s. Among the supporting players is Louella Parsons, playing herself (and, despite some negative comments I\\'ve seen, she has a very ingratiating personality on screen and a natural command of her lines). She is not the only real person in the script. Make-up specialist Perc Westmore briefly appears as himself to try to make one character resemble another.<br /><br />This film also was one of the first in the career of young Mr. Ronald Reagan, playing a radio interviewer at a movie premiere. Reagan actually does quite nicely in his brief scenes - particularly when he realizes that nobody Dick Powell is about to take over the microphone when it should be used with more important people.<br /><br />Dick Powell has won a Hollywood contract in a contest, and is leaving his job as a saxophonist in Benny Goodman\\'s band. The beginning of this film, by the way, is quite impressive, as the band drives in a parade of trucks to give a proper goodbye to Powell. They end up singing \"Hooray For Hollywood\". The interesting thing about this wonderful number is that a lyric has been left out on purpose. Throughout the Johnny Mercer lyrics are references to such Hollywood as Max Factor the make-up king, Rin tin tin, and even a hint of Tarzan. But the original song lyric referred to looking like Tyrone Power. Obviously Jack Warner and his brothers were not going to advertise the leading man of 20th Century Fox, and the name Donald Duck was substituted. In any event the number showed the singers and instrumentalists of Goodman\\'s orchestra at their best. So did a later five minute section of the film, where the band is rehearsing.<br /><br />Powell leaves the band and his girl friend (Frances Langford) and goes to Hollywood, only to find he is a contract player (most likely for musicals involving saxophonists). He is met by Allen Joslyn, the publicist of the studio (the owner is Grant Mitchell). Joslyn is not a bad fellow, but he is busy and he tends to slough off people unless it is necessary to speak to them. He parks Powell at a room at the Hollywood Hotel, which is also where the studio\\'s temperamental star (Lola Lane) lives with her father (Hugh Herbert), her sister (Mabel Todd), and her sensible if cynical assistant (Glenda Farrell). Lane is like Jean Hagen in \"Singing In The Rain\", except her speaking voice is good. Her version of \"Dan Lockwood\" is one \"Alexander Dupre\" (Alan Mowbray, scene stealing with ease several times). The only difference is that Mowbray is not a nice guy like Gene Kelly was, and Lane (when not wrapped up in her ego) is fully aware of it. Having a fit on being by-passed for an out-of-the ordinary role she wanted, she refuses to attend the premiere of her latest film. Joslyn finds a double for her (Lola\\'s real life sister Rosemary Lane), and Rosemary is made up to play the star at the premiere and the follow-up party. But she attends with Powell (Joslyn wanting someone who doesn\\'t know the real Lola). This leads to Powell knocking down Mowbray when the latter makes a pest of himself. But otherwise the evening is a success, and when the two are together they start finding each other attractive.<br /><br />The complications deal with Lola coming back and slapping Powell in the face, after Mowbray complains he was attacked by Powell (\"and his gang of hoodlums\"). Powell\\'s contract is bought out. Working with photographer turned agent Ted Healey (actually not too bad in this film - he even tries to do a Jolson imitation at one point), the two try to find work, ending up as employees at a hamburger stand run by bad tempered Edgar Kennedy (the number of broken dishes and singing customers in the restaurant give Edgar plenty of time to do his slow burns with gusto). Eventually Powell gets a \"break\" by being hired to be Dupre\\'s singing voice in a rip-off of \"Gone With The Wind\". This leads to the final section of the film, when Rosemary Lane, Herbert, and Healey help give Powell his chance to show it\\'s his voice, not Mowbrays.<br /><br />It\\'s quite a cute and appealing film even now. The worst aspects are due to it\\'s time. Several jokes concerning African-Americans are no longer tolerable (while trying to photograph Powell as he arrives in Hollywood, Healey accidentally photographs a porter, and mentions to Joslyn to watch out, Powell photographs too darkly - get the point?). Also a bit with Curt Bois as a fashion designer for Lola Lane, who is (shall we say) too high strung is not very tolerable either. Herbert\\'s \"hoo-hoo\"ing is a bit much (too much of the time) but it was really popular in 1937. And an incident where Healey nearly gets into a brawl at the premiere (this was one of his last films) reminds people of the tragic, still mysterious end of the comedian in December 1937. But most of the film is quite good, and won\\'t disappoint the viewer in 2008.'\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "8OGSq6-Mgusn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word_piece(ds, vocab_size, reserved_tokens):\n",
        "    word_piece_ds = ds.unbatch().map(lambda x, y: x)\n",
        "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "        word_piece_ds.batch(1000).prefetch(2),\n",
        "        vocabulary_size=vocab_size,\n",
        "        reserved_tokens=reserved_tokens,\n",
        "    )\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "QySzkc7sgwvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reserved_tokens = [\"[PAD]\", \"[UNK]\"]\n",
        "train_sentences = [element[0] for element in train_ds]\n",
        "vocab = train_word_piece(train_ds, VOCAB_SIZE, reserved_tokens)"
      ],
      "metadata": {
        "id": "azM8SFgAg6qH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens: \", vocab[100:110])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "og2PGifZg86Z",
        "outputId": "237a2366-81ff-417b-c5f5-a1e3bfcaf707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['¤', '¦', '§', '¨', '«', '®', '°', '³', '´', '·']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    lowercase=False,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        ")"
      ],
      "metadata": {
        "id": "eBKwyZxYg_Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sentence_ex = train_ds.take(1).get_single_element()[0][0]\n",
        "input_tokens_ex = tokenizer(input_sentence_ex)\n",
        "\n",
        "print(\"Sentence: \", input_sentence_ex)\n",
        "print(\"Tokens: \", input_tokens_ex)\n",
        "print(\"Recovered text after detokenizing: \", tokenizer.detokenize(input_tokens_ex))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP4HY470hBdv",
        "outputId": "21c6d9b5-d708-4c87-e15f-7d7c0f6a1ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence:  tf.Tensor(b'This picture seemed way to slanted, it\\'s almost as bad as the drum beating of the right wing kooks who say everything is rosy in Iraq. It paints a picture so unredeemable that I can\\'t help but wonder about it\\'s legitimacy and bias. Also it seemed to meander from being about the murderous carnage of our troops to the lack of health care in the states for PTSD. To me the subject matter seemed confused, it only cared about portraying the military in a bad light, as A) an organzation that uses mind control to turn ordinary peace loving civilians into baby killers and B) an organization that once having used and spent the bodies of it\\'s soldiers then discards them to the despotic bureacracy of the V.A. This is a legitimate argument, but felt off topic for me, almost like a movie in and of itself. I felt that \"The War Tapes\" and \"Blood of my Brother\" were much more fair and let the viewer draw some conclusions of their own rather than be beaten over the head with the film makers viewpoint. F-', shape=(), dtype=string)\n",
            "Tokens:  tf.Tensor(\n",
            "[  207   646   650   256   166    84 11467   261    13   170     8    84\n",
            "   402   176   241   176   163 13552  3991   165   163   381  4987    76\n",
            "  8445   184   195   301   501   167    83  1985   300   169  5499    15\n",
            "   199  8707    66   646   200  3886  2661  1845  1475  1402   171    42\n",
            "   210     8    85   511   180   807   203   170     8    84  4685  8918\n",
            "  8924   164  9582    15   635   170   650   166   558  2117   196   278\n",
            "   203   163  5177  8837   165   452  5475   166   163   769   165  4915\n",
            "   652   169   163  4133   178    49 11740  1188    15   590   231   163\n",
            "  1060   739   650  1827    13   170   222  4281   203  2686   163  1574\n",
            "   169    66   241   852    13   176    34    10   194   202  4009  9324\n",
            "   171  1317   507  1380   166   653  2301  3427  1994 11294   238  1384\n",
            "  2974   164    35    10   194 10544   171   508   474   516   164  1279\n",
            "   163  2775   165   170     8    84  1682   271  5166  8911   253   166\n",
            "   163 11312  8766   761    67  1856  7231 14310   165   163    55    15\n",
            "    34    15   207   167    66  8546  4470    13   180   610   288  3518\n",
            "   178   231    13   402   198    66   175   169   164   165   593    15\n",
            "    42   610   171     3   174  1173  7388   463     3   164     3  2317\n",
            "   165   232  5494     3   230   234   211  1641   164   524   163   687\n",
            "  2899   209  9394   165   225   365   430   233   188  4185   283   163\n",
            "   621   177   163   179  1394  9886    15    39    14     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0], shape=(512,), dtype=int32)\n",
            "Recovered text after detokenizing:  tf.Tensor(b'This picture seemed way to slanted , it \\' s almost as bad as the drum beating of the right wing kooks who say everything is rosy in Iraq . It paints a picture so unredeemable that I can \\' t help but wonder about it \\' s legitimacy and bias . Also it seemed to meander from being about the murderous carnage of our troops to the lack of health care in the states for PTSD . To me the subject matter seemed confused , it only cared about portraying the military in a bad light , as A ) an organzation that uses mind control to turn ordinary peace loving civilians into baby killers and B ) an organization that once having used and spent the bodies of it \\' s soldiers then discards them to the despotic bureacracy of the V . A . This is a legitimate argument , but felt off topic for me , almost like a movie in and of itself . I felt that \" The War Tapes \" and \" Blood of my Brother \" were much more fair and let the viewer draw some conclusions of their own rather than be beaten over the head with the film makers viewpoint . F - [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Formatting DataSet**"
      ],
      "metadata": {
        "id": "l52ux-3xhEiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(sentence, label):\n",
        "    sentence = tokenizer(sentence)\n",
        "    return ({\"input_ids\": sentence}, label)\n",
        "\n",
        "\n",
        "def make_dataset(dataset):\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(512).prefetch(16).cache()\n",
        "\n",
        "\n",
        "train_ds = make_dataset(train_ds)\n",
        "val_ds = make_dataset(val_ds)\n",
        "test_ds = make_dataset(test_ds)"
      ],
      "metadata": {
        "id": "zEDFtMJ9hECa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Model Buildeing***"
      ],
      "metadata": {
        "id": "_AwozLb8hNd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = keras.Input(shape=(None,), dtype=\"int64\", name=\"input_ids\")\n",
        "\n",
        "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")(input_ids)\n",
        "\n",
        "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
        "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
        "x = keras_nlp.layers.FNetEncoder(intermediate_dim=INTERMEDIATE_DIM)(inputs=x)\n",
        "\n",
        "\n",
        "x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "x = keras.layers.Dropout(0.1)(x)\n",
        "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "fnet_classifier = keras.Model(input_ids, outputs, name=\"fnet_classifier\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jns1YKXvhQOD",
        "outputId": "45a40a0f-8167-4bb3-dfae-662763e2d091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'f_net_encoder' (of type FNetEncoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "4zh-GMONhbc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnet_classifier.summary()\n",
        "fnet_classifier.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "fnet_classifier.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "rT_edDBthbKJ",
        "outputId": "cd2b5aa3-ff8a-4c0f-e2c2-fc969f073e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"fnet_classifier\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"fnet_classifier\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids (\u001b[38;5;33mInputLayer\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │       \u001b[38;5;34m1,985,536\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ f_net_encoder (\u001b[38;5;33mFNetEncoder\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m132,224\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ f_net_encoder_1 (\u001b[38;5;33mFNetEncoder\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m132,224\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ f_net_encoder_2 (\u001b[38;5;33mFNetEncoder\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m132,224\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,985,536</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ f_net_encoder (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FNetEncoder</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">132,224</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ f_net_encoder_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FNetEncoder</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">132,224</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ f_net_encoder_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">FNetEncoder</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">132,224</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling1d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,382,337\u001b[0m (9.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,382,337</span> (9.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,382,337\u001b[0m (9.09 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,382,337</span> (9.09 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:934: UserWarning: Layer 'position_embedding' (of type PositionEmbedding) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1070s\u001b[0m 3s/step - accuracy: 0.6141 - loss: 0.6285 - val_accuracy: 0.8544 - val_loss: 0.3345\n",
            "Epoch 2/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1077s\u001b[0m 3s/step - accuracy: 0.8774 - loss: 0.2967 - val_accuracy: 0.7808 - val_loss: 0.5647\n",
            "Epoch 3/3\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1043s\u001b[0m 3s/step - accuracy: 0.9267 - loss: 0.1871 - val_accuracy: 0.8436 - val_loss: 0.4587\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x786de8e7ce50>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculation**"
      ],
      "metadata": {
        "id": "vG9OaL60hpiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fnet_classifier.evaluate(test_ds, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "206M3ZcuhsKC",
        "outputId": "b2216b64-1e49-40dc-92d1-d927657ed2de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 1s/step - accuracy: 0.8302 - loss: 0.4726\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4625360667705536, 0.8331999778747559]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}